---
title: "LFAV"
layout: default
---

<!-- <div style="position: fixed; bottom: 15px; right:1px;">
  <a href=""> <img src="{{ site.baseurl }}/static/img/logo/cn.png" width="50%"; /> </a>
</div> -->

<!-- About -->
<section class="bg-light" id="about">
  <div class="container">
    
<!--     <div class="row">
      <div class="col-lg-12">
        <video autoplay muted loop width="100%"> -->
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm"> -->
<!--             <source src="{{ site.baseurl }}/static/videos/st_avqa.mp4" type="video/mp4">
              <source src="{{ site.baseurl }}/static/videos/st_avqa.mp4" type="video/mp4">
                Sorry, we cannot display the LFAV video wall as
                your browser doesn't support HTML5 video.
        </video>
      </div>
    </div>
 -->


<!--     <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase" style="text-align:center;">Learning to Answer Questions in Dynamic Audio-Visual Scenarios</h3>
        <h5 class="text-muted" style="text-align:center;">
          <ul>
              Guangyao Li<sup>1,&dagger;</sup>, 
              Yake Wei<sup>1,&dagger;</sup>, 
              Yapeng Tian<sup>2,&dagger;</sup>, 
              Chenliang Xu<sup>2</sup>, 
              Ji-Rong Wen<sup>1</sup>, 
              Di Hu<sup>1,*</sup><br>
              <sup>1</sup>Renmin University of China, 
              <sup>2</sup>University of Rochester            
          </ul>
        </h5>
        <p class="text-muted" style="text-align:center; color:#00F">
          <a href="{{ site.baseurl }}/static/files/LFAV.pdf">[Paper]&nbsp;
            <a href="{{ site.baseurl }}/static/files/LFAV-supp.pdf">[Supplementary]</a>&nbsp;
            <a href="{{ site.baseurl }}/static/files/LFAV-poster.pdf">[Poster]</a>&nbsp;
            <a href="https://www.youtube.com/watch?v=jn_4iabJcZw">[Video]</a>&nbsp;
            <a href="https://www.bilibili.com/video/BV1Br4y1q7YN/">[Video]</a>&nbsp;
            <a href="https://github.com/">[Code]</a>
        </p>
      </div>
    </div>
    <hr /> -->



<!-- 
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">Update</h2>
        <h5 class="text-muted" style="text-align:left;">
          <ul> -->
              <!-- <li>01 Jun 2022: The dataset has been uploaded to <a href="https://drive.google.com/drive/folders/1WAryZZE0srLIZG8VHl22uZ3tpbGHtsrQ?usp=sharing"><b>Google Drive</b></a>, welcome to download and use!</li>
              <li>28 Mar 2022: Camera-ready version has been released <a href="{{ site.baseurl }}/static/files/LFAV.pdf">here</a>!</li>
              <li>22 Mar 2022: The LFAV dataset has been released, please see <font color="danger">Download</font> for details.</li>
              <li>18 Mar 2022: Code has been released <a href="https://github.com/GeWu-Lab/LFAV">here</a>!</li>
              <li>08 Mar 2022: Watch the project's video demonstration on 
                <a href="https://www.youtube.com/watch?v=JH3t5gwe9Xw">YouTube</a> or 
                <a href="https://www.bilibili.com/video/BV1Br4y1q7YN/">Bilibili</a>.</li> -->
              <!-- <li>04 Jun 2023: Our paper is submitted to NeurIPS'2023. Code and dataset will be released soon!</li> -->
<!--               <li>06 Jun 2023: Paper, Code and Dataset will be released soon!</li>
          </ul>
        </h5>
      </div>  
    </div> -->

    <br/>

    
    <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase">Why Long form Audio-visual Video Understanding?</h3>
          <p  class="text-muted">
            We live in a world filled with never-ending streams of multimodal information.
            Videos captured from natural scenes have two typical characteristics: 
            <b>1) Long form</b>. They usually span several minutes, covering multiple related events in different categories. These events usually jointly contribute to depicting the main content of the video.
            <b>2) Audio-visual</b>. Videos recorded in real-world scenarios usually comprise both audio and visual modalities. These two aspects often exhibit asynchrony, providing unique perspectives in delineating the video content, yet collaboratively facilitating video understanding.
            <!--As a more natural recording of the real scenario, 
              long form audio-visual videos are expected as an important bridge for better exploring and understanding the world. 
              In this paper, we propose the multisensory temporal event localization task in long form videos and aim to address the accompanying challenges. 
            The multisensory temporal event localization task in long form audio-visual videos aims at precisely 
            localizing modality-aware events in the videos with several minutes long. 
            Some previous works on weakly supervised temporal action localization divide the video into several non-overlapping snippets, 
            then snippets are classified by multiple instance learning and aggregated to events. We follow this paradigm to define the task, 
            the output of the task is event categories of all snippets. Then event-level predictions can be directly generated by
            concatenating consecutive snippets with the same class of events.-->

            <!-- We focus on <b><font color="blue">audio-visual question answering (AVQA) task, which aims to answer questions regarding different visual objects, sounds, and their associations in videos</font></b>. The problem requires comprehensive multimodal understanding and spatio-temporal reasoning over audio-visual scenes. -->
          </p>
          <center><img src="{{ site.baseurl }}/static/img/avqa/lfav_teaser.png" alt="" style="width:99%;  margin-top:8px; margin-bottom:15px;"></center>
      </div>
    </div>

    <div class="row">
      <div class="col-md-12">
        <!-- <ul> -->
          <p class="text-muted">
            We show an example of long form audio-visual videos, with a length of 121-second. 
            This video shows a badminton game. The audio modality contains three events: 
            <b>cheering</b>, <b>clapping</b> and <b>speech</b>, the visual modality contains five events: 
            <b>playing badminton</b>, <b>cheering</b>, <b>crying</b>, <b>laughing</b>, and <b>clapping</b>. 
            The event <b>cheering</b> and <b>clapping</b> appears both in audio modality and visual modality. 
            These modality-aware events as well as their inherent relations help to effectively infer 
            what happens in the video then achieve a better understanding of the video content. 
            Considering the merits of the above two characteristics, we propose to study video understanding in terms of long form and audio-visual aspect, 
            name as \emph{long form audio-visual video understanding. 
          </p>
          <!-- <p class="text-muted">
            This video is 121-second long, consisting of various audio events and visual events, they either only occur in one modality or occur in both modalities but have different temporal boundaries. These modality-aware events as well as their inherent relations help to effectively infer what happens in the video then achieve a better understanding of the video content. Considering the merits of the above two characteristics, we propose to study video understanding in terms of long form and audio-visual aspect, name as \emph{long form audio-visual video understanding}. .
          </p> -->
        <!-- </ul> -->
      </div>
    </div>

    <br/>


    <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase">What is the multisensory temporal event localization task?</h3>
        <!-- <h5 class="text-muted" style="text-align:left;"> -->
          <!-- <ul> -->
            <p class="text-muted">
              <!-- To explore scene understanding and spatio-temporal reasoning over audio and visual modalities, we build a largescale audio-visual dataset, LFAV, which focuses on question-answering task. As noted above, high-quality datasets are of considerable value for AVQA research. -->
              <b>Task Definition.</b> To achieve a better understanding of long form audio-visual videos, we propose to focus on the multisensory temporal event localization task, 
              which essentially requires the model to predict the start and end time of each audio and visual event in the video. 
              Concretely, we divide the video into several non-overlapping snippets, then predict the event categories of all snippets.
              <!-- However, there remain several challenges when addressing this task. -->
            </p>
            <p class="text-muted">
              <!-- To explore scene understanding and spatio-temporal reasoning over audio and visual modalities, we build a largescale audio-visual dataset, LFAV, which focuses on question-answering task. As noted above, high-quality datasets are of considerable value for AVQA research. -->
              <b>Challenges.</b> Firstly, the video contains multiple events with diverse categories, modalities, and varying lengths. 
              Secondly, understanding the video content requires effectively modeling long-range dependencies and relations across different clips and modalities.
              <!-- However, there remain several challenges when addressing this task. -->
            </p>

          <!-- </ul> -->
        </h5>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <h3 class="section-heading text-uppercase">What is LFAV dataset?</h3>
        <!-- <h5 class="text-muted" style="text-align:left;"> -->
          <!-- <ul> -->
            <p class="text-muted">
              To study the proposed multisensory temporal event localization task, 
              we elaborately build a large-scale Long Form Audio-visual Video (LFAV) dataset with 5,175 videos, 
              as existing datasets are not appropriate for our proposed task. 
              Information and highlights of the LFAV dataset are shown below.
            </p>


          <!-- </ul> -->
        </h5>
      </div>
    </div>



    <div class="row">
      <!-- news column -->
      <div class="col-md-4">
        <h4 class="service-heading">Basic informations</h4>
        <p class="text-muted">We collect videos from YouTube, covering five kinds of daily life to ensure
           the diversity, complexity, and dynamic of the real world: 
           human-related, sports, musical instruments, tools, and animals. 
           We also construct a label set of 35 kinds of events covering the above scenes, </p>

      </div>
      <!-- characteristics column -->
      <div class="col-md-4">
        <h4 class="service-heading">Characteristics</h4>
        <ul class="text-muted">
          <li class="text-muted">5,175 videos</li>
          <li class="text-muted">average length of <b>210 seconds</b> and total length of <b>302 hours</b></li>
          <li class="text-muted">average event categories of <b>3.15</b> per video.</li>
          <li class="text-muted"><b>Modality-aware annotations</b> for all videos</li>
          <li class="text-muted">Diversity, complexity and dynamic</li>
        </ul>
      </div>

      <!-- udated column -->
      <div class="col-md-4">
        <h4 class="service-heading">Personal data/Human subjects</h4>
        <!-- <ul class="text-muted"> -->
          <p class="text-muted">Videos in LFAV are public on YouTube, and annotated via crowdsourcing. We have explained how the data would be used to crowdworkers. Our dataset does not contain personally identifiable information or offensive content.</p>
        <!-- </ul> -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12">
        <video autoplay muted loop width="100%">
          <!-- <source src="{{ site.baseurl }}/static/videos/03x03_videoWall.webm" type="video/webm"> -->
          <source src="{{ site.baseurl }}/static/videos/video.mp4" type="video/mp4">
              <!-- <source src="{{ site.baseurl }}/static/videos/lfav.mp4" type="video/mp4">
                Sorry, we cannot display the LFAV video wall as your browser doesn't support HTML5 video.-->
        </video>
      </div>
    </div>
    <!-- video banner row -->
      
       


  </div>
</section>

<!-- Stats -->
<section id="stats">
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading text-uppercase">LFAV Dataset</h2>
        <h3 class="section-subheading text-muted">Illustrations of our LFAV dataset statistics</h3>
      </div>
    </div>



      
    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat1.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left"><b>Illustrations of our LFAV dataset statistics</b>. 
        <b>(a-d)</b> Statistical analysis of label categories, including the distribution of event numbers in each video; 
        the distribution of video length; 
        the proportion of the top 4 event categories, 
        the top 4 labels represent <i>speech, clapping, cheering,</i> and <i>laughing</i>, which are the most common human actions; 
        the temporal proportion of events that occur on two modalities at the same time.
        <b>(e)</b> Second-order interactions between all labels, the thicker the line, the closer the association. 
        <b>(f)</b> Distribution of dataset labels black of each category.
        <!-- <b>(a-d)</b> statistical analysis of the videos and QA pairs. <b>(e)</b> Question formulas. <b>(f)</b> Distribution of question templates, where the dark color indicates the number of QA pairs generated from real videos while the light-colored area on the upper part of each bar means that from synthetic videos. <b>(g)</b> Distribution of first n-grams in questions. Our QA-pairs need <b>fine-grained scene understanding</b> and <b>spatio-temporal reasoning</b> over audio and visual modalities to be solved. For example, existential and location questions require spatial reasoning, and temporal questions require temporal reasoning. Best viewed in color.</p> -->
    </div>


    <!-- <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/matrix.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left"><b>Left: </b>Number of combinations of different types of instruments, where the lighter the color, the more the number. And instruments outside the 22 instrument categories are denoted by other. The confusion matrix shows that the combination of different instruments is diversified. <b>Right-top: </b> According to Wikipedia, 22 kinds of instruments are divided into 4 categories: <i>String, Wind, Percussion</i> and <i>Keyboard</i>. <b>Right-bottom: </b>9 question types in different scenarios.</p>
    </div> -->


    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/comp_others_nocite.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left">
        <b>Comparison with other datasets</b>. 
        Our LFAV dataset is collected for the proposed multisensory temporal event localization task, 
        where diversified domains are covered. 
        Specifically, the LFAV dataset offers modality-aware annotations for each video, 
        that is it points out the events are from audio, visual, or both modalities.
        Meanwhile, multiple events with different semantic categories per video 
        are also annotated for better exploring the relation among events. 
        Videos in the dataset have an average length of 210 seconds and a total length of 302 hours.
        (* means LLP only provides modality-aware annotations in validation and testing sets.)
      </p>
    </div>

    <br/>


<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" id="downloadFiles" style="text-align:left; margin-left:-14px">How was LFAV dataset made?</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        We design an audio-visual question answering labeling system to collect questions, and all QA pairs are collected with this system. The flow chart of the labeling system is shown in below figure.
      </p>
      <center>
        <img src="{{ site.baseurl }}/static/img/stats-figures/annotation_framework.png" alt="" style="width:88%;  margin-top:10px; margin-bottom:10px;"> 
      </center>
      <p class="text-muted" style="text-align:left">
        Labeling system contains <b>questioning</b> and <b>answering</b>. In the questioning section, the annotator is required to select the performance type of the video and the included instruments, and then scene types, question types, and question templates, and finally one question is automatically generated based on the previous selection. In the answering part, the annotator to judge whether the question is reasonable, and if it is unreasonable, the question will be labeled again. Then, the annotator answering the question according to video content, and finally one QA pair is produced.
      </p>
    </div>

    <br/> -->


<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-left:-14px">QA pairs samples</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        <b>Demo.</b> The large-scale spatial-temporal audio-visual dataset that focuses on question-answering task, as shown in below figure that different audio-visual scene types and their annotated QA pairs in the AVQA dataset.
      </p>

      <hr/>

      <div class="col-md centered" style="padding:0.3rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/st_avqa_pairs.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted", style="text-align:left">
        In the first row, <b>a)</b>, <b>b)</b>, and <b>c)</b> represent real musical performance videos, namely solo, ensemble of the same instrument, and ensemble of different instruments. In the second row, <b>d)</b>, <b>e)</b>, and <b>f)</b> represent the synthetic video, which are audio and video random matching, audio overlay, and video stitching, respectively.
      </p>
    </div> -->


    <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-left:-14px">Video examples</h4>
      </div>
      <hr/>
      <p class="text-muted" style="text-align:left">
        Some video examples in the LFAV dataset. Each of them contains multiple modality aware events. 
      </p>
      <p class="text-muted" style="text-align:left; margin-right: 10px;">
 
        <!-- example 1 ----------------------------------------------------------------- -->
        <table>
          <tr>

<!--             <td style="width:1%"></td>
            <td style="width:30%">
              <video width="90%" controls="controls">
                <source src="{{ site.baseurl }}/static/videos/demo1.mp4" align="left" type="video/mp4">
              </video>
            </td> -->

            <td style="width:1%"></td>
            <td style="width:30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/-6v1PqgFrDk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/DBejGH1-UCI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/1KpR1Dssbr8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

          </tr>
          
          <!-- visual event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  -6v1PqgFrDk 0 36  speech<br>
                  -6v1PqgFrDk 20  415 playing_ping-pong<br>
                  -6v1PqgFrDk 39  47  speech<br>
                  -6v1PqgFrDk 52  59  speech<br>
                  -6v1PqgFrDk 63  87  speech<br>
                  -6v1PqgFrDk 94  103 speech<br>
                  -6v1PqgFrDk 108 122 speech<br>
                  -6v1PqgFrDk 127 187 speech<br>
                  -6v1PqgFrDk 197 231 speech<br>
                  -6v1PqgFrDk 240 251 speech<br>
                  -6v1PqgFrDk 258 268 speech<br>
                  -6v1PqgFrDk 280 362 speech<br>
                  -6v1PqgFrDk 367 377 speech<br>
                  -6v1PqgFrDk 380 415 speech<br>
                </font>                
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  DBejGH1-UCI 0 314 piano<br>
                  DBejGH1-UCI 18  27  laughter<br>
                  DBejGH1-UCI 34  65  laughter<br>
                  DBejGH1-UCI 77  86  laughter<br>
                  DBejGH1-UCI 111 154 laughter<br>
                  DBejGH1-UCI 175 185 laughter<br>
                  DBejGH1-UCI 208 216 laughter<br>
                  DBejGH1-UCI 228 248 laughter<br>
                  DBejGH1-UCI 267 303 laughter<br>
                  DBejGH1-UCI 297 298 speech<br>
                  DBejGH1-UCI 315 339 speech<br>
                  DBejGH1-UCI 332 336 laughter<br>
                  DBejGH1-UCI 342 346 laughter<br>
                  DBejGH1-UCI 341 342 speech<br>
                  DBejGH1-UCI 350 352 speech<br>
                  DBejGH1-UCI 358 362 speech<br>
                  DBejGH1-UCI 369 372 speech<br>
                  DBejGH1-UCI 375 381 speech<br>
                  DBejGH1-UCI 387 390 speech<br>
                  DBejGH1-UCI 394 396 speech<br>
                  DBejGH1-UCI 397 417 laughter<br>
                  DBejGH1-UCI 402 403 speech<br>
                  DBejGH1-UCI 406 408 speech<br>
                  DBejGH1-UCI 410 414 speech<br>
                  DBejGH1-UCI 418 423 speech<br>
                  DBejGH1-UCI 428 430 speech<br>
                  DBejGH1-UCI 431 433 speech<br>
                  DBejGH1-UCI 431 435 laughter<br>
                </font> 
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  2P4jRC6DYzI 9 176 banjo<br>
                  2P4jRC6DYzI 9 176 drum<br>
                  2P4jRC6DYzI 4 14  piano<br>
                  2P4jRC6DYzI 17  48  car<br>
                  2P4jRC6DYzI 22  46  singing<br>
                  2P4jRC6DYzI 31  36  piano<br>
                  2P4jRC6DYzI 61  86  singing<br>
                  2P4jRC6DYzI 89  93  speech<br>
                  2P4jRC6DYzI 89  95  laughter<br>
                  2P4jRC6DYzI 111 142 singing<br>
                </font> 
              </div>
            </td>
          </tr>

          <!-- audio event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  -6v1PqgFrDk 0 36  speech<br>
                  -6v1PqgFrDk 24  39  playing_ping-pong<br>
                  -6v1PqgFrDk 39  47  speech<br>
                  -6v1PqgFrDk 48  62  playing_ping-pong<br>
                  -6v1PqgFrDk 52  59  speech<br>
                  -6v1PqgFrDk 63  87  speech<br>
                  -6v1PqgFrDk 74  83  playing_ping-pong<br>
                  -6v1PqgFrDk 87  93  playing_ping-pong<br>
                  -6v1PqgFrDk 94  103 speech<br>
                  -6v1PqgFrDk 104 107 playing_ping-pong<br>
                  -6v1PqgFrDk 108 122 speech<br>
                  -6v1PqgFrDk 123 126 playing_ping-pong<br>
                  -6v1PqgFrDk 127 187 speech<br>
                  -6v1PqgFrDk 141 144 playing_ping-pong<br>
                  -6v1PqgFrDk 159 162 playing_ping-pong<br>
                  -6v1PqgFrDk 167 169 playing_ping-pong<br>
                  -6v1PqgFrDk 174 177 playing_ping-pong<br>
                  -6v1PqgFrDk 180 196 playing_ping-pong<br>
                  -6v1PqgFrDk 197 231 speech<br>
                  -6v1PqgFrDk 200 201 playing_ping-pong<br>
                  -6v1PqgFrDk 230 238 playing_ping-pong<br>
                  -6v1PqgFrDk 240 251 speech<br>
                  -6v1PqgFrDk 252 257 playing_ping-pong<br>
                  -6v1PqgFrDk 258 268 speech<br>
                  -6v1PqgFrDk 269 279 playing_ping-pong<br>
                  -6v1PqgFrDk 280 362 speech<br>
                  -6v1PqgFrDk 283 287 playing_ping-pong<br>
                  -6v1PqgFrDk 292 304 playing_ping-pong<br>
                  -6v1PqgFrDk 357 366 playing_ping-pong<br>
                  -6v1PqgFrDk 367 377 speech<br>
                  -6v1PqgFrDk 378 394 playing_ping-pong<br>
                  -6v1PqgFrDk 380 415 speech<br>
                </font>
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  DBejGH1-UCI 0 3 cheering<br>
                  DBejGH1-UCI 3 5 laughter<br>
                  DBejGH1-UCI 1 3 speech<br>
                  DBejGH1-UCI 5 9 speech<br>
                  DBejGH1-UCI 10  13  speech<br>
                  DBejGH1-UCI 8 14  cheering<br>
                  DBejGH1-UCI 8 13  clapping<br>
                  DBejGH1-UCI 17  20  laughter<br>
                  DBejGH1-UCI 21  297 piano<br>
                  DBejGH1-UCI 20  23  speech<br>
                  DBejGH1-UCI 36  58  singing<br>
                  DBejGH1-UCI 59  165 singing<br>
                  DBejGH1-UCI 165 173 clapping<br>
                  DBejGH1-UCI 177 285 singing<br>
                  DBejGH1-UCI 286 302 clapping<br>
                  DBejGH1-UCI 297 298 speech<br>
                  DBejGH1-UCI 299 303 cheering<br>
                  DBejGH1-UCI 301 329 speech<br>
                  DBejGH1-UCI 330 340 speech<br>
                  DBejGH1-UCI 331 334 laughter<br>
                  DBejGH1-UCI 340 350 cheering<br>
                  DBejGH1-UCI 341 343 laughter<br>
                  DBejGH1-UCI 350 398 speech<br>
                  DBejGH1-UCI 398 402 cheering<br>
                  DBejGH1-UCI 402 404 speech<br>
                  DBejGH1-UCI 404 406 cheering<br>
                  DBejGH1-UCI 406 438 speech<br>
                  DBejGH1-UCI 408 413 cheering<br>
                  DBejGH1-UCI 419 438 clapping<br>
                  DBejGH1-UCI 433 436 cheering<br>
                  DBejGH1-UCI 436 438 speech<br>
                </font>
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  2P4jRC6DYzI 0 174 banjo<br>
                  2P4jRC6DYzI 0 174 drum<br>
                  2P4jRC6DYzI 22  45  singing<br>
                  2P4jRC6DYzI 62  85  singing<br>
                  2P4jRC6DYzI 111 142 singing<br>
                  2P4jRC6DYzI 117 118 speech<br>
                  2P4jRC6DYzI 174 176 cheering<br>
                  2P4jRC6DYzI 174 176 clapping<br>
                </font>
              </div>
            </td>
          </tr>


          <tr>
            <td>
              <div style="height: 20px;"></div>
            </td>
          </tr>

        </table>

        <br>



        <!-- example 2 ----------------------------------------------------------------- -->
        <table>
          <tr>
            <td style="width:1%"></td>
            <td style="width:30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/08hzunIk81Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/F9-W5ZCxDhs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/_-sfoqUa0vs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

          </tr>
          
          <!-- visual event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  08hzunIk81Y	7	31	bicycle<br>
                  08hzunIk81Y	7	8	car<br>
                  08hzunIk81Y	10	17	car<br>
                  08hzunIk81Y	20	21	car<br>
                  08hzunIk81Y	27	57	car<br>
                  08hzunIk81Y	47	48	dance<br>
                  08hzunIk81Y	52	53	dog<br>
                  08hzunIk81Y	58	60	bicycle<br>
                  08hzunIk81Y	59	60	car<br>
                  08hzunIk81Y	61	62	speech<br>
                  08hzunIk81Y	62	94	car<br>
                  08hzunIk81Y	63	76	bicycle<br>
                  08hzunIk81Y	80	81	bicycle<br>
                  08hzunIk81Y	86	94	bicycle<br>
                  08hzunIk81Y	92	93	speech<br>
                  08hzunIk81Y	95	96	speech<br>
                  08hzunIk81Y	96	104	bicycle<br>
                  08hzunIk81Y	98	104	car<br>
                  08hzunIk81Y	107	109	bicycle<br>
                  08hzunIk81Y	107	109	car<br>
                  08hzunIk81Y	111	121	bicycle<br>
                  08hzunIk81Y	111	119	car<br>
                  08hzunIk81Y	122	123	bicycle<br>
                  08hzunIk81Y	125	126	bicycle<br>
                  08hzunIk81Y	129	137	bicycle<br>
                  08hzunIk81Y	131	136	car<br>
                  08hzunIk81Y	136	137	laughter<br>
                  08hzunIk81Y	138	151	bicycle<br>
                  08hzunIk81Y	138	144	car<br>
                  08hzunIk81Y	140	142	laughter<br>
                  08hzunIk81Y	140	142	clapping<br>
                  08hzunIk81Y	153	175	bicycle<br>
                  08hzunIk81Y	155	156	car<br>
                  08hzunIk81Y	160	163	car<br>
                  08hzunIk81Y	171	173	car<br>
                  08hzunIk81Y	174	175	speech<br>
                  08hzunIk81Y	183	196	bicycle<br>
                  08hzunIk81Y	185	186	speech<br>
                  08hzunIk81Y	190	192	laughter<br>
                  08hzunIk81Y	190	192	clapping<br>
                  08hzunIk81Y	194	200	car<br>
                  08hzunIk81Y	197	198	speech<br>
                  08hzunIk81Y	203	204	speech<br>
                  08hzunIk81Y	204	225	bicycle<br>
                  08hzunIk81Y	209	213	car<br>
                  08hzunIk81Y	217	225	car<br>
                  08hzunIk81Y	226	227	laughter<br>
                  08hzunIk81Y	227	228	bicycle<br>
                  08hzunIk81Y	227	232	car<br>
                  08hzunIk81Y	230	275	bicycle<br>
                  08hzunIk81Y	246	271	car<br>
                  08hzunIk81Y	276	288	bicycle<br>
                  08hzunIk81Y	281	288	car<br>
                  08hzunIk81Y	289	290	car<br>
                  08hzunIk81Y	297	347	speech<br>
                  08hzunIk81Y	297	347	car<br>
                  08hzunIk81Y	300	305	dog<br>
                  08hzunIk81Y	318	321	dog<br>
                  08hzunIk81Y	335	336	bicycle<br>
                </font>                
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  F9-W5ZCxDhs 8 18  helicopter<br>
                  F9-W5ZCxDhs 36  194 helicopter<br>
                  F9-W5ZCxDhs 63  152 speech<br>
                  F9-W5ZCxDhs 199 204 speech<br>
                  F9-W5ZCxDhs 204 283 helicopter<br>
                  F9-W5ZCxDhs 310 439 speech<br>
                  F9-W5ZCxDhs 443 480 helicopter<br>
                </font> 
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  _-sfoqUa0vs 0 2 violin<br>
                  _-sfoqUa0vs 0 6 speech<br>
                  _-sfoqUa0vs 3 4 laughter<br>
                  _-sfoqUa0vs 7 10  clapping<br>
                  _-sfoqUa0vs 11  15  violin<br>
                  _-sfoqUa0vs 16  17  clapping<br>
                  _-sfoqUa0vs 18  19  laughter<br>
                  _-sfoqUa0vs 19  22  violin<br>
                  _-sfoqUa0vs 23  26  clapping<br>
                  _-sfoqUa0vs 27  29  violin<br>
                  _-sfoqUa0vs 27  29  speech<br>
                  _-sfoqUa0vs 32  35  violin<br>
                  _-sfoqUa0vs 32  35  speech<br>
                  _-sfoqUa0vs 39  60  violin<br>
                  _-sfoqUa0vs 42  52  speech<br>
                  _-sfoqUa0vs 60  61  laughter<br>
                  _-sfoqUa0vs 62  65  violin<br>
                  _-sfoqUa0vs 73  74  laughter<br>
                  _-sfoqUa0vs 74  76  speech<br>
                  _-sfoqUa0vs 75  76  violin<br>
                  _-sfoqUa0vs 90  95  laughter<br>
                  _-sfoqUa0vs 90  93  laughter<br>
                  _-sfoqUa0vs 95  99  violin<br>
                  _-sfoqUa0vs 103 106 laughter<br>
                  _-sfoqUa0vs 107 117 violin<br>
                  _-sfoqUa0vs 111 112 speech<br>
                  _-sfoqUa0vs 113 117 speech<br>
                  _-sfoqUa0vs 122 124 speech<br>
                  _-sfoqUa0vs 122 141 violin<br>
                  _-sfoqUa0vs 145 152 violin<br>
                  _-sfoqUa0vs 153 154 laughter<br>
                  _-sfoqUa0vs 155 161 violin<br>
                  _-sfoqUa0vs 157 166 laughter<br>
                  _-sfoqUa0vs 168 174 violin<br>
                  _-sfoqUa0vs 175 180 laughter<br>
                  _-sfoqUa0vs 178 180 violin<br>
                  _-sfoqUa0vs 180 182 clapping<br>
                  _-sfoqUa0vs 183 192 violin<br>
                  _-sfoqUa0vs 197 203 violin<br>
                  _-sfoqUa0vs 204 205 cry<br>
                  _-sfoqUa0vs 206 209 violin<br>
                  _-sfoqUa0vs 210 214 laughter<br>
                  _-sfoqUa0vs 211 214 clapping<br>
                  _-sfoqUa0vs 223 247 violin<br>
                  _-sfoqUa0vs 248 250 laughter<br>
                  _-sfoqUa0vs 251 255 violin<br>
                  _-sfoqUa0vs 259 262 violin<br>
                  _-sfoqUa0vs 263 264 laughter<br>
                  _-sfoqUa0vs 265 268 violin<br>
                  _-sfoqUa0vs 274 277 violin<br>
                  _-sfoqUa0vs 277 278 laughter<br>
                  _-sfoqUa0vs 279 316 violin<br>
                  _-sfoqUa0vs 317 318 laughter<br>
                  _-sfoqUa0vs 319 326 violin<br>
                  _-sfoqUa0vs 327 328 laughter<br>
                  _-sfoqUa0vs 329 333 violin<br>
                  _-sfoqUa0vs 334 335 laughter<br>
                  _-sfoqUa0vs 336 338 violin<br>
                  _-sfoqUa0vs 339 340 laughter<br>
                  _-sfoqUa0vs 342 343 cello<br>
                  _-sfoqUa0vs 344 351 violin<br>
                  _-sfoqUa0vs 352 354 cello<br>
                  _-sfoqUa0vs 355 368 violin<br>
                  _-sfoqUa0vs 369 370 laughter<br>
                  _-sfoqUa0vs 371 373 violin<br>
                  _-sfoqUa0vs 377 384 violin<br>
                  _-sfoqUa0vs 385 386 laughter<br>
                  _-sfoqUa0vs 387 394 violin<br>
                  _-sfoqUa0vs 395 398 cello<br>
                  _-sfoqUa0vs 399 403 violin<br>
                  _-sfoqUa0vs 405 408 violin<br>
                  _-sfoqUa0vs 409 413 clapping<br>
                  _-sfoqUa0vs 420 431 clapping<br>
                  _-sfoqUa0vs 420 421 cello<br>
                  _-sfoqUa0vs 425 428 violin<br>
                  _-sfoqUa0vs 432 436 violin<br>
                  _-sfoqUa0vs 437 438 laughter<br>
                  _-sfoqUa0vs 442 443 clapping<br>
                  _-sfoqUa0vs 442 443 laughter<br>
                  _-sfoqUa0vs 448 449 violin<br>
                  _-sfoqUa0vs 452 454 laughter<br>
                  _-sfoqUa0vs 452 453 clapping<br>
                  _-sfoqUa0vs 455 457 violin<br>
                  _-sfoqUa0vs 455 457 speech<br>
                  _-sfoqUa0vs 459 460 clapping<br>
                  _-sfoqUa0vs 461 483 violin<br>
                  _-sfoqUa0vs 475 477 clapping<br>
                  _-sfoqUa0vs 484 486 laughter<br>
                  _-sfoqUa0vs 485 490 violin<br>
                  _-sfoqUa0vs 490 495 laughter<br>
                  _-sfoqUa0vs 492 514 violin<br>
                  _-sfoqUa0vs 515 516 cello<br>
                  _-sfoqUa0vs 515 522 clapping<br>
                  _-sfoqUa0vs 523 524 violin<br>
                  _-sfoqUa0vs 528 530 laughter<br>
                  _-sfoqUa0vs 528 530 clapping<br>
                  _-sfoqUa0vs 531 540 violin<br>
                  _-sfoqUa0vs 541 542 speech<br>
                  _-sfoqUa0vs 543 546 violin<br>
                  _-sfoqUa0vs 547 548 drum<br>
                  _-sfoqUa0vs 549 550 violin<br>
                  _-sfoqUa0vs 551 552 drum<br>
                  _-sfoqUa0vs 552 553 violin<br>
                  _-sfoqUa0vs 554 556 drum<br>
                  _-sfoqUa0vs 558 559 violin<br>
                  _-sfoqUa0vs 567 569 violin<br>
                  _-sfoqUa0vs 571 573 violin<br>
                  _-sfoqUa0vs 576 578 violin<br>
                  _-sfoqUa0vs 580 594 violin<br>
                  _-sfoqUa0vs 592 594 laughter<br>
                  _-sfoqUa0vs 599 608 violin<br>
                  _-sfoqUa0vs 608 609 drum<br>
                  _-sfoqUa0vs 612 613 violin<br>
                  _-sfoqUa0vs 621 622 drum<br>
                  _-sfoqUa0vs 622 623 violin<br>
                  _-sfoqUa0vs 625 626 violin<br>
                  _-sfoqUa0vs 633 644 horse<br>
                  _-sfoqUa0vs 645 646 laughter<br>
                  _-sfoqUa0vs 646 648 horse<br>
                  _-sfoqUa0vs 649 650 violin<br>
                  _-sfoqUa0vs 653 654 drum<br>
                  _-sfoqUa0vs 653 654 clapping<br>
                  _-sfoqUa0vs 653 656 horse<br>
                  _-sfoqUa0vs 660 671 horse<br>
                  _-sfoqUa0vs 666 667 drum<br>
                  _-sfoqUa0vs 673 675 violin<br>
                  _-sfoqUa0vs 676 677 drum<br>
                  _-sfoqUa0vs 680 682 horse<br>
                  _-sfoqUa0vs 683 687 violin<br>
                  _-sfoqUa0vs 688 691 drum<br>
                  _-sfoqUa0vs 692 693 violin<br>
                  _-sfoqUa0vs 694 695 horse<br>
                  _-sfoqUa0vs 698 699 violin<br>
                  _-sfoqUa0vs 699 700 horse<br>
                  _-sfoqUa0vs 704 707 horse<br>
                  _-sfoqUa0vs 708 715 violin<br>
                  _-sfoqUa0vs 716 730 horse<br>
                  _-sfoqUa0vs 723 725 clapping<br>
                  _-sfoqUa0vs 730 736 clapping<br>
                  _-sfoqUa0vs 737 742 violin
                </font> 
              </div>
            </td>
          </tr>

          <!-- audio event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  08hzunIk81Y	5	23	drum<br>
                  08hzunIk81Y	23	24	cat<br>
                  08hzunIk81Y	25	103	drum<br>
                  08hzunIk81Y	103	104	car<br>
                  08hzunIk81Y	104	106	speech<br>
                  08hzunIk81Y	107	210	drum<br>
                  08hzunIk81Y	131	135	cheering<br>
                  08hzunIk81Y	136	137	laughter<br>
                  08hzunIk81Y	139	143	cheering<br>
                  08hzunIk81Y	140	143	clapping<br>
                  08hzunIk81Y	150	151	cheering<br>
                  08hzunIk81Y	177	181	speech<br>
                  08hzunIk81Y	187	193	cheering<br>
                  08hzunIk81Y	190	193	clapping<br>
                  08hzunIk81Y	210	211	speech<br>
                  08hzunIk81Y	218	292	drum<br>
                  08hzunIk81Y	295	346	speech<br>
                </font>
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  F9-W5ZCxDhs 8 18  helicopter<br>
                  F9-W5ZCxDhs 30  35  car<br>
                  F9-W5ZCxDhs 36  480 helicopter<br>
                  F9-W5ZCxDhs 396 404 dog<br>
                </font>
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  _-sfoqUa0vs 0 6 speech<br>
                  _-sfoqUa0vs 6 32  cheering<br>
                  _-sfoqUa0vs 6 37  clapping<br>
                  _-sfoqUa0vs 27  39  speech<br>
                  _-sfoqUa0vs 40  41  laughter<br>
                  _-sfoqUa0vs 42  52  speech<br>
                  _-sfoqUa0vs 53  55  speech<br>
                  _-sfoqUa0vs 56  57  speech<br>
                  _-sfoqUa0vs 63  64  speech<br>
                  _-sfoqUa0vs 71  74  laughter<br>
                  _-sfoqUa0vs 73  83  clapping<br>
                  _-sfoqUa0vs 75  80  speech<br>
                  _-sfoqUa0vs 85  86  speech<br>
                  _-sfoqUa0vs 90  95  laughter<br>
                  _-sfoqUa0vs 90  96  clapping<br>
                  _-sfoqUa0vs 92  93  speech<br>
                  _-sfoqUa0vs 96  97  speech<br>
                  _-sfoqUa0vs 98  103 laughter<br>
                    _-sfoqUa0vs 103 109 speech<br>
                    _-sfoqUa0vs 102 105 clapping<br>
                    _-sfoqUa0vs 111 112 speech<br>
                    _-sfoqUa0vs 113 125 speech<br>
                    _-sfoqUa0vs 126 208 violin<br>
                    _-sfoqUa0vs 156 158 laughter<br>
                    _-sfoqUa0vs 158 159 clapping<br>
                    _-sfoqUa0vs 163 164 laughter<br>
                    _-sfoqUa0vs 164 165 clapping<br>
                    _-sfoqUa0vs 173 184 clapping<br>
                    _-sfoqUa0vs 210 217 clapping<br>
                    _-sfoqUa0vs 214 215 cheering<br>
                    _-sfoqUa0vs 224 408 violin<br>
                    _-sfoqUa0vs 247 249 laughter<br>
                    _-sfoqUa0vs 248 251 clapping<br>
                    _-sfoqUa0vs 334 335 clapping<br>
                    _-sfoqUa0vs 407 464 clapping<br>
                    _-sfoqUa0vs 408 450 cheering<br>
                    _-sfoqUa0vs 451 453 laughter<br>
                    _-sfoqUa0vs 455 458 speech<br>
                    _-sfoqUa0vs 458 460 cheering<br>
                    _-sfoqUa0vs 462 463 speech<br>
                    _-sfoqUa0vs 468 509 violin<br>
                    _-sfoqUa0vs 474 495 clapping<br>
                    _-sfoqUa0vs 494 495 laughter<br>
                    _-sfoqUa0vs 509 525 cheering<br>
                    _-sfoqUa0vs 509 530 clapping<br>
                    _-sfoqUa0vs 519 525 speech<br>
                    _-sfoqUa0vs 528 529 laughter<br>
                    _-sfoqUa0vs 529 537 speech<br>
                    _-sfoqUa0vs 539 541 speech<br>
                    _-sfoqUa0vs 542 559 drum<br>
                    _-sfoqUa0vs 568 570 violin<br>
                    _-sfoqUa0vs 572 574 violin<br>
                    _-sfoqUa0vs 576 578 violin<br>
                    _-sfoqUa0vs 580 624 violin<br>
                    _-sfoqUa0vs 609 610 drum<br>
                    _-sfoqUa0vs 609 629 singing<br>
                    _-sfoqUa0vs 613 614 drum<br>
                    _-sfoqUa0vs 617 618 drum<br>
                    _-sfoqUa0vs 621 624 drum<br>
                    _-sfoqUa0vs 630 640 clapping<br>
                    _-sfoqUa0vs 634 730 drum<br>
                    _-sfoqUa0vs 634 730 violin<br>
                    _-sfoqUa0vs 634 730 cello<br>
                    _-sfoqUa0vs 723 742 clapping<br>
                </font>
              </div>
            </td>
          </tr>

          <tr>
            <td>
              <div style="height: 20px;"></div>
            </td>
          </tr>

        </table>

        <br>




        <!-- example 3 ----------------------------------------------------------------- -->
        <table>
          <tr>
            <td style="width:1%"></td>
            <td style="width:30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/9s0T5-rPOZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/AIFOZsn-bFE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

            <td style="width:1%"></td>
            <td style="width: 30%">
              <iframe width="368" height="207" src="https://www.youtube.com/embed/iHgFgZ-YPJY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </td>

          </tr>
          
          <!-- visual event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  9s0T5-rPOZo 22  32  alarm<br>
                  9s0T5-rPOZo 37  40  bicycle<br>
                  9s0T5-rPOZo 41  92  alarm<br>
                  9s0T5-rPOZo 107 114 alarm<br>
                  9s0T5-rPOZo 119 126 alarm<br>
                  9s0T5-rPOZo 128 140 alarm<br>
                  9s0T5-rPOZo 141 142 alarm<br>
                  9s0T5-rPOZo 149 150 alarm<br>
                  9s0T5-rPOZo 154 179 alarm<br>
                  9s0T5-rPOZo 213 224 alarm<br>
                  9s0T5-rPOZo 241 271 alarm<br>
                </font>                
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  AIFOZsn-bFE 0 9 speech<br>
                  AIFOZsn-bFE 0 9 frisbee<br>
                  AIFOZsn-bFE 9 14  dog<br>
                  AIFOZsn-bFE 12  21  frisbee<br>
                  AIFOZsn-bFE 14  40  speech<br>
                  AIFOZsn-bFE 25  27  frisbee<br>
                  AIFOZsn-bFE 40  109 frisbee<br>
                  AIFOZsn-bFE 40  45  dog<br>
                  AIFOZsn-bFE 45  46  speech<br>
                  AIFOZsn-bFE 46  50  dog<br>
                  AIFOZsn-bFE 50  52  speech<br>
                  AIFOZsn-bFE 52  109 dog<br>
                  AIFOZsn-bFE 56  78  speech<br>
                  AIFOZsn-bFE 89  92  speech<br>
                  AIFOZsn-bFE 94  100 speech<br>
                  AIFOZsn-bFE 103 107 speech<br>
                  AIFOZsn-bFE 109 109 clapping<br>
                  AIFOZsn-bFE 110 112 speech<br>
                  AIFOZsn-bFE 112 113 frisbee<br>
                  AIFOZsn-bFE 112 113 dog<br>
                  AIFOZsn-bFE 113 113 clapping<br>
                  AIFOZsn-bFE 116 118 frisbee<br>
                  AIFOZsn-bFE 116 119 dog<br>
                  AIFOZsn-bFE 119 121 speech<br>
                  AIFOZsn-bFE 122 124 speech<br>
                  AIFOZsn-bFE 124 131 dog<br>
                  AIFOZsn-bFE 125 131 frisbee<br>
                  AIFOZsn-bFE 126 169 speech<br>
                </font> 
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#148F77">
                  <b>Visual Event Labels</b><br>
                  iHgFgZ-YPJY 16  270 dance<br>
                  iHgFgZ-YPJY 16  286 laughter<br>
                  iHgFgZ-YPJY 90  130 cheering<br>
                  iHgFgZ-YPJY 90  98  clapping<br>
                  iHgFgZ-YPJY 123 132 clapping<br>
                  iHgFgZ-YPJY 157 158 cheering<br>
                  iHgFgZ-YPJY 157 161 clapping<br>
                  iHgFgZ-YPJY 270 280 cheering<br>
                  iHgFgZ-YPJY 271 285 clapping<br>
                </font> 
              </div>
            </td>
          </tr>

          <!-- audio event labels -->
          <tr>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  9s0T5-rPOZo 9 27  speech<br>
                  9s0T5-rPOZo 28  147 alarm<br>
                  9s0T5-rPOZo 117 118 speech<br>
                  9s0T5-rPOZo 156 272 speech<br>
                  9s0T5-rPOZo 252 283 piano<br>
                </font>
              </div>                      
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  AIFOZsn-bFE 0 116 speech<br>
                  AIFOZsn-bFE 108 109 clapping<br>
                  AIFOZsn-bFE 113 113 clapping<br>
                  AIFOZsn-bFE 117 119 cheering<br>
                  AIFOZsn-bFE 119 119 clapping<br>
                  AIFOZsn-bFE 120 123 speech<br>
                  AIFOZsn-bFE 125 169 speech<br>
                </font>
              </div>
            </td>
            <td style="width:1%"></td>
            <td style="width: 30%">
              <div style="height: 120px; text-align:left; border: solid 0.5px #DBDCDB; overflow-x:auto ;overflow-y: auto;">
                <font color="#2193D6">
                  <b>Audio Event Labels</b><br>
                  iHgFgZ-YPJY 0 16  piano<br>
                  iHgFgZ-YPJY 2 76  singing<br>
                  iHgFgZ-YPJY 17  27  laughter<br>
                  iHgFgZ-YPJY 17  98  piano<br>
                  iHgFgZ-YPJY 20  28  speech<br>
                  iHgFgZ-YPJY 35  37  laughter<br>
                  iHgFgZ-YPJY 54  77  guitar<br>
                  iHgFgZ-YPJY 62  77  drum<br>
                  iHgFgZ-YPJY 78  289 cheering<br>
                  iHgFgZ-YPJY 88  170 drum<br>
                  iHgFgZ-YPJY 97  103 singing<br>
                  iHgFgZ-YPJY 111 229 singing<br>
                  iHgFgZ-YPJY 176 178 banjo<br>
                  iHgFgZ-YPJY 171 179 clapping<br>
                  iHgFgZ-YPJY 179 180 piano<br>
                  iHgFgZ-YPJY 179 229 drum<br>
                  iHgFgZ-YPJY 212 268 piano<br>
                  iHgFgZ-YPJY 235 270 singing<br>
                  iHgFgZ-YPJY 239 268 drum<br>
                  iHgFgZ-YPJY 274 289 clapping<br>
                </font>
              </div>
            </td>
          </tr>

        </table>





        

        
      </p>

    </div>


<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" id="downloadFiles">QA pairs samples</h4>
      </div>
      <hr/> -->
    <!--   <div class="text-muted" style="text-align:left">
      
      </div> -->
<!--       <hr/>
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/st_avqa_pairs.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left">
        The large-scale spatial-temporal audio-visual dataset that focuses on question-answering task, as shown in:
      </p>
    </div> -->

<!--     <div class="row justify-content-md-center text-center">
    <div class="col-md-12">
      <h4 class="section-subheading" id="downloadFiles">How was LFAV dataset made?</h4>
        <div class="col-md centered" style="padding:1rem;">
          <img src="{{ site.baseurl }}/static/img/stats-figures/annotation_framework.png" style="width: 100%" class="img-responsive"/> 
        </div>
        <p>
          <b>Flow chart of the labeling system.</b> Labeling system contains questioning and answering. In the questioning section, the annotator is required to select the performance type of the video and the included instruments, and then scene types, question types, and question templates, and finally one question is automatically generated based on the previous selection. In the answering part, the annotator to judge whether the question is reasonable, and if it is unreasonable, the question will be labeled again. Then, the annotator answering the question according to video content, and finally one QA pair is produced.
        </p>
        <hr/>
    </div>
    </div> -->


    <!-- <br/><br/> -->
<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat2.png" style="width: 100%" class="img-responsive"/> 
      </div>
    </div>

    <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat3.png" style="width: 100%" class="img-responsive"/> 
        <h4>Distribution</h4>
      </div>
    </div> -->
      


      <!-- <div class="col-md-6 centered" style="padding:1rem; vertical-align:bottom"> -->
        <!-- TO ADD GRAPH: replace div below, ex: above <img> tag -->
        <!-- <img src="{{ site.baseurl }}/static/img/stats-figures/masks.png" style="width: 100%" class="img-responsive"/>   -->
        <!-- <h4>Automatic Annotations</h4> -->
      <!-- </div> -->
    </div>

    <!-- <div class="row justify-content-md-center text-center">
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph4" style="width: 100%" class="img-responsive"></div>
      <h4>Resolution</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph5" style="width: 100%" class="img-responsive"></div>
      <h4>Number of Frames</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph6" style="width: 100%" class="img-responsive"></div>
      <h4>Total number of hours</h4>
      </div>
      </div>
      <div class="row justify-content-md-center text-center">
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph7" style="width: 100%" class="img-responsive"></div>
      <h4>Number of annotators<br/>used per video</h4>
      </div>
      <div class="col-md-4 centered" style="padding:1rem;">
      <div id="graph8" style="width: 100%" class="img-responsive"></div>
      <h4>Splits</h4>
      </div>
      </div> -->

      <!-- <div class="col-md-6">
        <div class="card" style="border: solid 2px; background-color: #373435ff; margin-bottom:5px;">
        <h1 style=" color: white; text-decoration: underline; text-decoration-color: #ed323eff;"> Baseline Models </h1>
        <div id="graph9"></div>
        </div>

        <div class="card" style="border: solid 2px; background-color: #ed323eff;">
        <h1 style=" color: white; text-decoration: underline; text-decoration-color: #373435ff;"> State of the Art Results </h1>
        <div id="graph10"></div>
        </div>

        </div> -->
  </div>
</section>




<section class="bg-light" id="downloads">
  <div class="container">
    
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">Download</h2>
        <h3 class="section-subheading text-muted">Dataset publicly available for research purposes</h3>
      </div>
    </div>


    <div class="row">
      <div class="col-md-12"> 
        
        <h4 class="section-subheading" id="downloadFiles">Data download </h4><hr/>
<!--         <P>Raw videos:
          <ul>
            <li>
              <a href="https://gewu-lab.github.io/LFAV/">Google Drive</a> (Comming soon!)
            </li>
            <li>Baidu Drive (<b>password: lfav</b>)</li>
            - <a href="https://pan.baidu.com/s/1yVPfOXyDesHdUZFHK3tYog">Real videos</a> (36.67GB) <br/>
            - <a href="https://pan.baidu.com/s/1b7HQbMdcfaWjsHdWiLWO2Q">Synthetic videos</a> (11.59GB) <br/>
            <b>Note</b>: Please move all downloaded videos to a folder, for example, create a new folder named LFAV-Videos, which contains 9,288 real videos and synthetic videos.
          </ul> 
        </P> -->
          
        <!-- <p>YouTube URL of raw videos: comming soon.</p> -->
        
        <p>Extracted features:
          <ul>
            <li>VGGish feature shape: [T, 128]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://drive.google.com/file/d/1bvTBotLHnPGIeIAkkgMWK7wcWjZ5xbfo/view?usp=drive_link">Google Drive</a> or&nbsp;
              <a href="https://pan.baidu.com/s/1nSdhEilGxGFs-7FOgsDoFw">Baidu Drive</a> (pwd: lfav),&nbsp;(~662M)</li>
            <li>ResNet18 feature shape: [T, 512]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://drive.google.com/file/d/14p4jgDo-tteeZPzRBbEq1982tT-uxviZ/view?usp=drive_link">Google Drive</a> or&nbsp;
              <a href="https://pan.baidu.com/s/1GAstblAMXbhlUj_8QD_ONg">Baidu Drive</a> (pwd: lfav),&nbsp;(~2.6G)</li>
            <li>R(2+1)D feature shape: [T, 512]&nbsp;&nbsp;, Download from&nbsp;
              <a href="https://drive.google.com/file/d/1FfLpS0PLPXNJ28SqqYLb_vBATlUWnDvK/view?usp=drive_link">Google Drive</a> or&nbsp;
              <a href="https://pan.baidu.com/s/1-jRD7MQ0RT0lAN5DP40syA">Baidu Drive</a> (pwd: lfav),&nbsp;(~2.6G)</li>
            </ul>
        </p>

        <p> Annotations (train, val and test set): Available for download at <a href="https://github.com/GeWu-Lab/LFAV/tree/main/LFAV_dataset">GitHub</a>.</p>

        <p>
          <b>How to read the annotation files: </b>

          The annotation files are stored in CSV format. Each annotation file contains several different keyword, and we present a detailed explanation of them.
          <ul>
            <li>Train set: filename, video-level labels</li>
            <li>Val and Test set: filename, onset, offset, event-level labels</li>
          </ul>
        </p>
        
        <br/>
        <h4 class="section-subheading">Publication(s)</h4>
        <p>
          If you find our work useful in your research, please cite our paper.
        </p>
        <pre class="bibtex" style="text-align:left; margin-left:2px">
        <code>
        @ARTICLE{Comming soon!,
          title={Towards Long Form Audio-visual Video Understanding},
          author={Wenxuan Hou, Guangyao li, Yapeng Tian, Di Hu},
          journal={Comming soon!},
          year={Comming soon!},
        }</code>
        </pre>


        <br/>
        <h4 class="section-subheading">Disclaimer </h4>
        <!-- <p>The LFAV was collected as a tool for research in computer vision. The dataset may have unintended biases (including those of a societal, gender or racial nature).</p> -->
        <p> The released LFAV dataset is curated, which perhaps owns potential correlation between instrument and geographical area. This issue warrants further research and consideration.
        </p>

      </div>
    </div>

      
      <!-- <div class="row">
        <div class="col-md-12">
          <h4 class="section-subheading">Disclaimer </h4>
            <p>The LFAV was collected as a tool for research in computer vision. The dataset may have unintended biases (including those of a societal, gender or racial nature).</p>
            <p> The released LFAV dataset is curated, which perhaps owns potential correlation between instrument and geographical area. This issue warrants further research and consideration.
            </p>
        </div>
      </div> -->

    <br/>
    <div class="row">
      <div class="col-md-12">
        <h4 class="section-subheading">Copyright <img alt="Creative Commons License" style="border-width:1px;float:left;margin-right:15px;margin-bottom:0px;" src="https://i.creativecommons.org/l/by-nc/3.0/88x31.png"/></h4>
        <p>
          All datasets and benchmarks on this page are copyright by us and published under the <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International</a> License. This means that  you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. You may not use the material for commercial purposes.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- Benchmark -->
<!-- <section id="challenges">
  <div class="container">
    
    <div class="row">
      <div class="col-md-12 text-center">
        <h2 class="section-heading text-uppercase">An event-centric framework</h2> -->
        <!-- <h3 class="section-subheading text-muted">Challenge Details with links to &#9733;NEW&#9733; Codalab Leaderboards</h3> -->
        <!-- <h3 class="section-subheading text-muted">method, experimental results and simple analysis</h3>
      </div>
    </div>
     
    <div class="row">
      <div class="col-md-12">
        <p class="text-muted" style="text-align:left">
          We propose an event-centric framework containing three phases from snippet prediction, event extraction to event interaction. 
          Firstly, we propose a pyramid multimodal transformer model to learn snippet-level features by executing intra-modal and cross-modal interaction within multiscale temporal windows.
          Secondly, we extract event-level features by refining and aggregating event-aware snippet features in structured graphs.
          At last, we study event relations by modeling the influence among multiple audio and visual events, then refining the event features.
          The three phases are jointly optimized with video-level event labels in an end-to-end fashion. 
          <b>More details are in the <a href="{{ site.baseurl }}/static/files/LFAV.pdf">[Paper]</a> and <a href="{{ site.baseurl }}/static/files/LFAV-supp.pdf">[Supplementary]</a></b>.<br/>  
        </p> 
      </div>
    </div> -->
    

    <!-- <div class="row">
      <div class="col-md-12"> -->
        <!-- <h4 class="subheading">Spatio-temporal Grounding Model</h4>
        <p class="text-muted" style="text-align:left">An overview of the proposed framework is illustrated in below figure. We consider that the sound and the location of its visual source usually reflects the spatial association between audio and visual modality, the <b>spatial grounding module</b>, which performs attention-based sound source localization, is therefore introduced to decompose the complex scenarios into concrete audio-visual association. To highlight the key timestamps that are closely associated to the question, we propose a <b>temporal grounding module</b>, which is designed for attending critical temporal segments among the changing audio-visual scenes and capturing question-aware audio and visual embeddings.</p>
         -->
        <!-- <div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
        <img src="{{ site.baseurl }}/static/img/experiments/framework_pipeline_long.png" style="width: 100%;" class="img-responsive"/> 
        </div> -->
        <!-- <p class="text-muted" style="text-align:left">The <b>proposed audio-visual question answering model</b> takes pre-trained CNNs to extract audio and visual features and uses a LSTM to obtain a question embedding. We associate specific visual locations with the input sounds to perform spatial grounding, based on which audio and visual features of key timestamps are further highlighted via question query for temporal grounding. Finally, multimodal fusion is exploited to integrate audio, visual, and question information for predicting the answer to the input question. </p> -->
      <!-- </div>
    </div>


    <br/> -->

    <!-- <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Experiments</h4>
        <p class="text-muted" style="text-align:left">
          To validate the superiority of our proposed framework, we choose 13 related methods for comparison, 
          including weakly supervised temporal action localization methods: STPN, RSKP; 
          long sequence modeling methods: Longformer, Transformer-LS, ActionFormer; 
          audio-visual learning methods: AVE, AVSlowFast, HAN, PSP, DHHN; 
          video classification methods: SlowFast, MViT, and MeMViT. 
        </p>

        <div class="col-md centered" style="padding:0.6rem; text-align:center">
        <img src="{{ site.baseurl }}/static/img/experiments/exp1.png" style="width: 100%;" class="img-responsive"/> 
        </div>
        <p class="text-muted" style="text-align:left">
          <b>Comparison to Other Methods.</b> <b>Firstly</b>, temporal action localization and long sequence modeling methods
          aim to effectively localize action events in untrimmed videos or model long sequences.
          But they ignore the valuable cooperation among audio and video modality, which is important in achieving more comprehensive video event understanding. 
          <b>Secondly</b>, although some methods take the audio signal into account, they are consistently worse than our method. 
          This could be because they mainly aim at understanding trimmed short videos, resulting in limited modeling of long-range dependencies and event interactions.
          <b>Thirdly</b>, our proposed method outperforms all the comparison ones obviously, although some recent video classification methods
           achieve slightly better results on visual mAP, their overall performance still lags obviously behind our proposed method, 
           showing that our proposed event-centric framework can localize both audio and visual events in long form audio-visual videos better. 
        </p>
        <p class="text-muted" style="text-align:left">
          <b>Effectiveness of Three Phases.</b> As mentioned above,
           our full method consists of three progressive phases. 
           The performance of the snippet prediction phase has already surpassed most comparison methods, 
           then the subsequent phases can further improve localization performance. 
           Results are shown in the last three rows of the above Table, 
           which indicate the potential importance of decoupling a long form audio-visual video into multiple uni-modal events with different lengths and modeling their inherent relations in both uni-modal and cross-modal scenarios. 
        </p>  -->


        <!-- <p class="text-muted" style="text-align:left; margin-right: 10px;"> -->
          <!-- <img src="{{ site.baseurl }}/static/img/experiments/exp2.png" align="left" width="51%" class="img-responsive"/>  -->
          <!-- <img src="{{ site.baseurl }}/static/img/experiments/exp4.png" align="right" width="100%" class="img-responsive"/>
        </p>
        <p> </p>
        <p>
          <b>Effectiveness of Feature Interaction.</b> We further explore the effectiveness of snippet-level and event-level feature interaction. Results are shown in two above tables, respectively.
          Both kinds of interactions benefit from intra-modal and cross-modal attention, which not only indicates the importance of the uni-modal sequence 
          but also shows the meaning of cross-modal relations in achieving effective long form audio-visual video understanding. 
        </p>
      </div>
    </div>


    <br/> -->

    <!-- <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">Visualized localization results</h4> -->
        <!-- <p class="text-muted" style="text-align:left">We provide several visualized spatial grounding results. The heatmap indicates the location of sounding source. Through the spatial grounding results, the sounding objects are visually captured, which can facilitate the spatial reasoning.</p> -->
        <!-- <div class="col-md centered" style="padding:0.2rem; text-align:center; margin-bottom:8px">
        <img src="{{ site.baseurl }}/static/img/experiments/visualize1.png" style="width: 100%;" class="img-responsive"/> 
        </div>
        <p class="text-muted" style="text-align:left">
          We visualize the event-level localization results in the videos, two examples are shown in the above figure. Compared with the audio-visual video parsing method HAN, our proposed method achieves better localization results.
           In some situations (<i>e.g.</i>, event <i>guitar</i> in both audio and visual modality of <i>video 01</i>, and event <i>speech</i> in the audio modality of <i>video 02</i>), 
          HAN tends to localize some sparse and short video clips instead of a long and complete event, which shows that HAN exists some limitations to understanding long-form videos. 
          The possible reason is that HAN cannot learn long-range dependencies well. 
        </p>
        <p class="text-muted" style="text-align:left">
          We also notice that, although our proposed event-centric method has achieved the best performance among all methods, there still exist some failure cases in the shown examples (red and black boxes in the figure).
          The multisensory events take huge different lengths and occur in a dynamic long-range scene, which makes multisensory temporal event localization become a very challenging task, 
          especially with only video-level labels in training. 
        </p>

      </div>
    </div> -->



    

<!--     <div class="row justify-content-md-center text-center">
      <div class="col-md centered" style="padding:1rem;">
        <img src="{{ site.baseurl }}/static/img/stats-figures/stat1.png" style="width: 100%" class="img-responsive"/> 
      </div>
      <p class="text-muted" style="text-align:left"><b>Illustrations of our LFAV dataset statistics</b>. <b>(a-d)</b> statistical analysis of the videos and QA pairs. <b>(e)</b> Question formulas. <b>(f)</b> Distribution of question templates, where the dark color indicates the number of QA pairs generated from real videos while the light-colored area on the upper part of each bar means that from synthetic videos. <b>(g)</b> Distribution of first n-grams in questions. Our QA-pairs need <b>fine-grained scene understanding</b> and <b>spatio-temporal reasoning</b> over audio and visual modalities to be solved. For example, existential and location questions require spatial reasoning, and temporal questions require temporal reasoning. Best viewed in color.</p>
    </div> -->
      
   
    
    <!-- <div class="row"> -->
      <!-- <div class="col-md-12"> -->
        <!-- <h4 class="subheading">Challenges/Leaderboard Details</h4>                    

        <p class="text-muted">        
        <b>Splits. </b> The dataset is split in train/validation/test sets, with a ratio of roughly 75/10/15. <br/>         
        The action recognition, detection and anticipation challenges use all the splits. <br/>        
        The unsupservised domain adaptation and action retrieval challenges use different splits as detailed below. <br/>        
        
        You can download all the necessary annotations <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations" target="_blank">here</a>. <br/>
        You can find more details about the splits in <a href="https://arxiv.org/pdf/2006.13256.pdf" target="_blank">our paper</a>.
        </p>
        
        <p class="text-muted">
        <b>Evaluation. </b> All challenges are evaluated considering all segments in the Test split. 
        The action recognition and anticipation challenges are additionally evaluated considering unseen participants and tail classes. These are automatically evaluated in the scripts and you do not need to do anything specific to report these.<br/>
        <b>Unseen participants. </b> The validation and test sets contain participants that are not present in the train set. 
        There are 2 unseen participants in the validation set, and another 3 participants in the test set. 
        The corresponding action segments are 1,065 and 4,110 respectively. <br/>
        <b>Tail classes. </b> These are the set of smallest classes whose instances account for 20&#37 of the total number of instances in 
        training. A tail action class contains either a tail verb class or a tail noun class. 
        <br/><br/>
        </p> -->
        
        
        
        <!-- <section class="challenge" id="challenge-action-detection">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Action Detection</h5>
              <p class="text-muted">
                <b>Task. </b> 
                Detect the start and the end of each action in an <i>untrimmed</i> video. Assign a (verb, noun) label to each 
                detected segment. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label.  <br/>                
                <b>Testing input. </b> A set of <i>untrimmed</i> videos. <u>Important:</u> You are not allowed to use the knowledge of trimmed segments in the test set when reporting for this challenge.<br/>
                <b>Splits. </b> Train and validation for training, evaluated on the test split. <br/>
                <b>Evaluation metrics. </b> Mean Average Precision (mAP) @ IOU 0.1 to 0.5.
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C2-Action-Detection">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C2-Action-Detection">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/707#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/707#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/707#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/707#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/ad.png" style="width: 100%" class="img-responsive"/> 
         </figure> 
        </section> -->
        

        <!-- <section class="challenge" id="challenge-action-anticipation">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Action Anticipation</h5>
              <p class="text-muted">
                <b>Task. </b> 
                Predict the (verb, noun) label of a future action observing a segment preceding its occurrence. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label. <br/>
                <b>Testing input. </b> During testing you are allowed to observe a segment that <i>ends</i> at least one second before 
                the start of the action you are testing on.<br/>                
                <b>Splits. </b> Train and validation for training, evaluated on the test split. <br/>
                <b>Evaluation metrics. </b> Top-5 recall averaged for all classes, as defined <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Furnari_Leveraging_Uncertainty_to_Rethink_Loss_Functions_and_Evaluation_Measures_for_ECCVW_2018_paper.pdf" target="_blank">here</a>,
                calculated for all segments as well as unseen participants and tail classes.
                <br/>
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C3-Action-Anticipation">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C3-Action-Anticipation">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/702#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/702#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/702#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/702#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/aa.png" style="width: 100%" class="img-responsive"/> 
         </figure>
        </section> -->
        
        <!-- <section class="challenge" id="challenge-domain-adaptation">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Unsupervised Domain Adaptation for Action Recognition</h5>
              <p class="text-muted">
                <b>Task. </b> Assign a (verb, noun) label to a trimmed segment, following the Unsupervised Domain Adaptation paradigm: 
                a labelled source domain is used for training, and the model needs to adapt to an unlabelled target domain. <br/>
                <b>Training input. </b> A set of trimmed action segments, each annotated with a (verb, noun) label.  <br/>                
                <b>Testing input. </b> A set of trimmed unlabelled action segments. <br/>
                <b>Splits. </b> Videos recorded in 2018 (EPIC-KITCHENS-55) constitute the source domain, 
                while videos recorded for LFAV's extension constitute the unlabelled target domain. 
                This challenge uses custom train/validation/test splits, which you can find 
                <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations#unsupervised-domain-adaptation-challenge" target="_blank">here</a>. <br/> 
                <b>Evaluation metrics. </b> Top-1/5 accuracy for verb, noun and action (verb+noun), on the target test set.
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C4-UDA-for-Action-Recognition">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C4-UDA-for-Action-Recognition">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/1241#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/uda.png" style="width: 100%" class="img-responsive"/> 
         </figure>
        </section>-->


        <!-- <section class="challenge" id="challenge-action-retrieval">
          <div class="row">
            <div class="col-md-12">
              <h5 class="subheading">Multi-Instance Retrieval</h5>
              <p class="text-muted">
                <b>Tasks. </b> <i>Video to text</i>: given a query video segment, rank captions such that those with a higher rank are 
                more semantically relevant to the action in the query video segment. 
                <i>Text to video:</i> given a query caption, rank video segments such that those with a higher rank are more semantically relevant 
                to the query caption. <br/>                                
                <b>Training input. </b> A set of trimmed action segments, each annotated with a caption. 
                Captions correspond to the narration in English from which the action segment was obtained. <br/>                
                <b>Testing input. </b> A set of trimmed action segments with captions. Important: You are not allowed to use the known correspondence in the Test set <br/>
                <b>Splits. </b> This challenge has its own custom splits, available <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations/tree/master/retrieval_annotations">here</a>. <br/>                
                <b>Evaluation metrics. </b> normalised Discounted Cumulative Gain (nDCG) and Mean Average Precision (mAP). 
                You can find more details in <a href="https://arxiv.org/pdf/2006.13256.pdf" target="_blank">our paper</a>. 
              </p>
            </div>
          </div>
          
          <div class="details row text-center">
            <div class="col-md">
              <a href="https://github.com/epic-kitchens/C5-Multi-Instance-Retrieval">
                <i class="text-center centered fas fa-4x fa-seedling"></i>
              </a>
              <br />
              <p class="text-muted">
                <a href="https://github.com/epic-kitchens/C5-Multi-Instance-Retrieval">Get started</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/617#learn_the_details-submission-format">
                <i class="text-center centered fas fa-4x fa-info"></i>
              </a>
              <br />
              <p class="text-muted">Learn about the 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/617#learn_the_details-submission-format">submission format details</a>
              </p>
            </div>
            <div class="col-md">
              <a href="https://codalab.lisn.upsaclay.fr/competitions/617#results">
                <i class="text-center centered fas fa-4x fa-trophy"></i>
              </a>
              <p class="text-muted">Submit your results on 
                <a href="https://codalab.lisn.upsaclay.fr/competitions/617#results">CodaLab website</a>
              </p>
            </div>
         </div>
         <figure class="text-muted">
            <figcaption>Sample qualitative results from the challenge's baseline</figcaption>
            <img src="{{ site.baseurl }}/static/img/challenges-epic-100/aret.png" style="width: 100%" class="img-responsive"/> 
         </figure>
          
        </section> -->
        
      <!-- </div>
    </div> -->
    
  </div>
</section> 

<!-- Team -->
<section class="bg-light" id="team">
  <div class="container">
    
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">The Team</h2>
        <div class="text-muted">
          <p> We are a group of researchers working in computer vision
            from the <a href="http://ai.ruc.edu.cn/">Renmin University of China</a> and <a href="https://www.utdallas.edu/"> University of Texas at Dallas</a>.
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-md-6">
        <a href="http://ai.ruc.edu.cn/">
          <img src="{{ site.baseurl }}/static/img/universities/ruc.png" alt="" style="width:80%;  margin-top:10px; margin-bottom:15px;">
        </a>
      </div>
      <div class="col-md-6">
        <a href="https://www.cs.rochester.edu/">
          <img src="{{ site.baseurl }}/static/img/universities/UTDallas.png" alt="" style="width:50%;  margin-top:20px; margin-bottom:15px;">
        </a>
      </div>
    </div>
    <hr>



<!--<div class="row">
      <div class="col-md-6">
        <div class="team-member">
          <a href="http://dimadamen.github.io">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/dd-min.jpg"/>

            <h4>Guangyao Li</h4></a>
            <h5>Renmin University of China</h5>
            <h6 class="text-muted">University of Bristol, United Kingom</h6>
        </div>
      </div>
        
      <div class="col-md-6">
        <div class="team-member">
          <a href="http://www.dmi.unict.it/farinella/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/gmf-min.jpg" />

            <h4>Giovanni Maria Farinella</h4></a>
          <h5>Co-Investigator</h5>
          <h6 class="text-muted">University of Catania, Italy</h6>
        </div>
      </div>
    </div>
-->
    

    <!-- <center> -->
    <div class="row" style="text-align:center">  

      <div class="col-md-3"> <!--Wenxuan Hou-->
        <div class="team-member">
          <a href="https://hou9612.github.io/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/wenxuanhou.jpg" />
            <h4>Wenxuan Hou</h4></a>
            <h5>PhD Student</h5>
            <h5>(Sep 2022 - )</h5>
            <h6 class="text-muted">Renmin University of China</h6>
        </div>
      </div>

      <div class="col-md-3"> <!--Guangyao Li-->
        <div class="team-member">
          <a href="https://ayameyao.github.io/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/guangyaoli.jpg" />
            <h4>Guangyao Li</h4></a>
            <h5>PhD Candidate</h5>
            <h5>(Sep 2020 - )</h5>
            <h6 class="text-muted">Renmin University of China</h6>
        </div>
      </div>

      <div class="col-md-3"><!--Yapeng Tian-->
        <div class="team-member">
          <a href="http://www.yapengtian.com/">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/yapengtian.jpg" />
            <h4>Yapeng Tian</h4></a>
            <h5>Assistant Professor</h5>
            <!-- <h5>(Sep 2017 - )</h5> -->
            <h6 class="text-muted">University of Texas at Dallas</h6>
        </div>
      </div>

      <div class="col-md-3"> <!--Di Hu-->
        <div class="team-member">
            <a href="https://dtaoo.github.io/index.html">
            <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/dihu.png" />
            <h4>Di Hu</h4></a>
            <h5>Assistant Professor</h5>
            <h6 class="text-muted">Renmin University of China</h6>
        </div>
      </div>


    </div>
  <!-- </center> -->
<!-- 
  <center>
    <div class="row">

    </div>
  </center> -->


    <div class="container">
      <div class="row">

<!--         <div class="col-lg-12">
          <h2 class="section-heading text-uppercase">Research Funding</h2>
          <div class="text-muted">
            <p> The work on LFAV was supported by the following research grants</p>
              <ul class="text-muted">
                <li>Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China.</li>
                <li>Beijing Outstanding Young Scientist Program (NO.BJJWZYJH012019100020098)</li>
                <li>Research Funds of Renmin University of China (NO.21XNLG17)</li>
                <li>National Natural Science Foundation of China (NO.62106272)</li>
                <li>2021 Tencent AI Lab Rhino-Bird Focused Research Program (No.JR202141)</li>
                <li>Young Elite Scientists Sponsorship Program by CAST</li>
                <li>Large-Scale Pre-Training Program of Beijing Academy of Artificial Intelligence (BAAI)</li>
              </ul>
          </div>
        </div> -->

        <!-- <div class="col-lg-12">
          <h2 class="section-heading text-uppercase">Acknowledgement</h2>
          <div class="text-muted">
              <ul class="text-muted">
                <li>This research was supported by Public Computing Cloud, Renmin University of China.</li> -->
                <!-- <li>This web-page design inspired by EPIC official website.</li> -->
              <!-- </ul>
          </div>
        </div> -->

      </div>
    </div>
</section>


<!--<section class="bg-light" id="results">
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h2 class="section-heading text-uppercase">Results - 2021 Challenges (June 2021)</h2>
        <div class="text-muted">
            <div class="row">
      <div class="col-md-12">
        <h4 class="subheading">EPIC-Kitchens Challenges @CVPR2021, Virtual CVPR</h4>
        <div class="row">
           <div class="col-md-3">
                Jan 1, 2021
            </div>
            <div class="col-md-9">
                EPIC-Kitchens Challenges 2020 Launched!
            </div>
        </div>
        <div class="row">
            <div class="col-md-3">
                June 1, 2021
            </div>
            <div class="col-md-9">
                Server Submission Deadline at 23:59:59 GMT
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                Jun 4, 2020
            </div>
            <div class="col-md-9">
                Deadline for Submission of Technical Reports
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                June 20, 2020
            </div>
            <div class="col-md-9">
                Results announced at <a href="https://eyewear-computing.org/EPIC_CVPR21/">EPIC@CVPR2021</a> Workshop (<a href="https://youtu.be/FSn8yCbpcc4">watch session recording here</a>)
            </div>
        </div>  
        <div class="row">
            <div class="col-md-3">
                July 6, 2021
            </div>
            <div class="col-md-9">
                Technical report for all submissions to the 2021 challenges is now <a href="Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">available here</a> [Reference <a href="./Reports/2021-bibtex.txt">Bibtex</a>].
            </div>
        </div>  
      </div>
    </div>
            
        <h2 class="section-heading text-uppercase">2021 Challenge Winners</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/winnersList-2021.png" width=100%/>
                  </div>
                   <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/winners-2021.png" width=100%/>
                  </div>
            
          <h2 class="section-heading text-uppercase">Action Recognition Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AR.png" width=100%/>
                  </div>
           <h2 class="section-heading text-uppercase">Action Anticipation Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AN.png" width=100%/>
                  </div>
          <h2 class="section-heading text-uppercase">Action Detection Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/AD.png" width=100%/>
                  </div>
             <h2 class="section-heading text-uppercase">Unsupervised Domain Adaptation for Action Recognition Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/UDA.png" width=100%/>
                  </div>
            
             <h2 class="section-heading text-uppercase">Multi-Instance Retrieval Challenge - 2021</h2> 
            
                  <div class="col-md">
                    <img src="{{site.baseurl}}/static/results2021/MIR.png" width=100%/>
                  </div>
        </div>
      </div>
    </div>
    </div>
</section>-->

<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"LFAV dataset",
  "description":"First-person (egocentric) video dataset; multi-faceted non-scripted recordings in the wearers' homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel live audio commentary approach.",
  "url":"https://github.com/epic-kitchens/annotations",
  "sameAs":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d",
  "citation":"Damen, Dima et al. 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset', European Conference on Computer Vision, 2018",
  "identifier": "10.5523/bris.3h91syskeag572hl6tvuovwv4d",
  "keywords":[
     "Egocentric vision",
     "Human actions",
     "Object interactions",
     "actions",
     "video",
     "kitchens",
     "cooking",
     "dataset",
     "epic kitchens",
     "epic",
     "eccv",
     "2022"
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://epic-kitchens.github.io/",
     "name":"EPIC Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"uob-epic-kitchens@bristol.ac.uk",
        "url":"https://github.com/epic-kitchens/annotations/issues"
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     }
  ],
  "license": "https://creativecommons.org/licenses/by-nc/4.0/"
}
</script>

